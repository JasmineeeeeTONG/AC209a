{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 6\n",
    "# Reg-Logistic Regression, ROC, and Data Imputation\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- Do not include your name(s) in the notebook if you are submitting as a group. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your partner's name (if you submit separately): **Chia Chi Ho (Michelle Ho)**\n",
    "\n",
    "Enrollment Status (109A, 121A, 209A, or E109A): **209A**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn.apionly as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Breast Cancer Detection\n",
    "\n",
    "In this homework, we will consider the problem of early breast cancer detection from X-ray images. Specifically, given a candidate region of interest (ROI) from an X-ray image of a patient's breast, the goal is to predict if the region corresponds to a malignant tumor (label 1) or is normal (label 0). The training and test data sets for this problem is provided in the file `hw6_dataset.csv`. Each row in these files corresponds to a ROI in a patient's X-ray, with columns 1-117 containing features computed using standard image processing algorithms. The last column contains the class label, and is based on a radiologist's opinion or a biopsy. This data was obtained from the KDD Cup 2008 challenge.\n",
    "\n",
    "The data set contain a total of 69,098 candidate ROIs, of which only 409 are malignant, while the remaining are all normal. \n",
    "\n",
    "*Note*: be careful of reading/treating column names and row names in this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Beyond Classification Accuracy\n",
    "\n",
    "\n",
    "0.  Split the data set into a training set and a testing set.  The training set should be 75% of the original data set, and the testing set 25%.  Use `np.random.seed(9001)`.\n",
    "\n",
    "1. Fit a logistic regression classifier to the training set and report the  accuracy of the classifier on the test set. You should use $L_2$ regularization in logistic regression, with the regularization parameter tuned using cross-validation. \n",
    "    1. How does the fitted model compare with a classifier that predicts 'normal' (label 0) on all patients? \n",
    "    2. Do you think the difference in the classification accuracies are large enough to declare logistic regression as a better classifier than the all 0's classifier? Why or why not?\n",
    "    \n",
    "For applications with imbalanced class labels, in this case when there are many more healthy subjects ($Y=0$) than those with cancer ($Y=1$), the classification accuracy may not be the best metric to evaluate a classifier's performance. As an alternative, we could analyze the confusion table for the classifier. \n",
    "\n",
    "<ol start=\"3\">\n",
    "<li> Compute the confusion table for both the fitted classifier and the classifier that predicts all 0's.</li>\n",
    "<li> Using the entries of the confusion table compute the *true positive rate* and the *true negative rate* for the two classifiers. Explain what these evaluation metrics mean for the specific task of cancer detection. Based on the observed metrics, comment on whether the fitted model is better than the all 0's classifier.</li>\n",
    "<li> What is the *false positive rate* of the fitted classifier, and how is it related to its true positive and true negative rate? Why is a classifier with high false positive rate undesirable for a cancer detection task?</li>\n",
    "</ol>\n",
    "*Hint:* You may use the `metrics.confusion_matrix` function to compute the confusion matrix for a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into TRAINING (default 0.75) & TEST set\n",
    "def split_hw6_data(df, n=117, seed=9001, train_size=0.75): # Use seed=9001 each time\n",
    "    np.random.seed(seed)\n",
    "    msk = np.random.rand(len(df)) < train_size\n",
    "    data_train = df[msk]\n",
    "    data_test = df[~msk]\n",
    "    \n",
    "    X_train = data_train.iloc[:, :n]\n",
    "    X_test = data_test.iloc[:, :n]\n",
    "    y_train = data_train.iloc[:, n]\n",
    "    y_test = data_test.iloc[:, n]\n",
    "    \n",
    "    print('X_train.shape={} \\nX_test.shape={} \\ny_train.shape={} \\ny_test=shape{}'.format(X_train.shape, X_test.shape, y_train.shape, y_test.shape))\n",
    "    \n",
    "    return (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_summary(name, test_acc, cm):\n",
    "    print('===================================')\n",
    "    print('{}'.format(name))\n",
    "    print('-----------------------------------')\n",
    "    print('Accuracy - Test: {}'.format(test_acc))\n",
    "    print('Confusion Matrix:')\n",
    "    print(cm)\n",
    "    print('True Positive Rate: {}'.format(cm[1,1]/(cm[1,0]+cm[1,1])))\n",
    "    print('True Negative Rate: {}'.format(cm[0,0]/(cm[0,0]+cm[0,1])))\n",
    "    print('False Positive Rate: {}'.format(cm[0,1]/(cm[0,0]+cm[0,1])))\n",
    "    print('===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=(52009, 117) \n",
      "X_test.shape=(17089, 117) \n",
      "y_train.shape=(52009,) \n",
      "y_test=shape(17089,)\n"
     ]
    }
   ],
   "source": [
    "df_full = pd.read_csv('hw6_dataset.csv', header=None)\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = split_hw6_data(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Model Predicting All Zeros\n",
      "-----------------------------------\n",
      "Accuracy - Test: 0.9942067996957107\n",
      "Confusion Matrix:\n",
      "[[16990     0]\n",
      " [   99     0]]\n",
      "True Positive Rate: 0.0\n",
      "True Negative Rate: 1.0\n",
      "False Positive Rate: 0.0\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# Model that predicts 0\n",
    "all0_test_acc = np.mean(1 - y_test_full)\n",
    "all0_cm = confusion_matrix(y_test_full, np.zeros(len(X_test_full)))\n",
    "model_summary('Model Predicting All Zeros', all0_test_acc, all0_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "LogisticRegressionCV w/ L2 Regularization\n",
      "-----------------------------------\n",
      "Accuracy - Test: 0.995026040142782\n",
      "Confusion Matrix:\n",
      "[[16984     6]\n",
      " [   79    20]]\n",
      "True Positive Rate: 0.20202020202020202\n",
      "True Negative Rate: 0.9996468510888759\n",
      "False Positive Rate: 0.0003531489111241907\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression \n",
    "log_regr_cv = LogisticRegressionCV(penalty='l2', scoring='accuracy')\n",
    "log_regr_cv.fit(X_train_full, y_train_full)\n",
    "log_cv_score_test = log_regr_cv.score(X_test_full, y_test_full)\n",
    "log_cm = confusion_matrix(y_test_full, log_regr_cv.predict(X_test_full))\n",
    "\n",
    "model_summary('LogisticRegressionCV w/ L2 Regularization', log_cv_score_test, log_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers for Question 1\n",
    "\n",
    "Report accuracy of on the test set: \n",
    "> **Model Predicting All Zeros:** \n",
    "   - **test_acc = 0.9942067996957107**\n",
    "\n",
    "> **Fitted LogisticRegression Classifier:** \n",
    "   - **test_acc = 0.995026040142782**\n",
    "\n",
    "How does the fitted model compare with a classifier that predicts 'normal' (label 0) on all patients? \n",
    "> **The fitted logistic regression model has similar accuracy compared to the model predicting all '0's. Both models have pretty high accuracy on the test set.**\n",
    "\n",
    "Do you think the difference in the classification accuracies are large enough to declare logistic regression as a better classifier than the all 0's classifier? Why or why not?\n",
    "> ** No, the difference is very small (~ 0.0009). With such a small magnitude, the difference in accuracy would most likely have no practical implications/ramifications. Therefore, we cannot declare logistic regression as a better classifier soley based on the accuracy on the test data.**\n",
    "\n",
    "Using the entries of the confusion table compute the *true positive rate* and the *true negative rate* for the two classifiers. \n",
    "> **Model Predicting All Zeros:**\n",
    "   - **true positive rate = 0.0**\n",
    "   - **true negative rate = 1.0**\n",
    "\n",
    "> **Fitted LogisticRegression Classifier:**\n",
    "   - **true positive rate = 0.20202020202020202**\n",
    "   - **true negative rate = 0.9996468510888759**\n",
    "\n",
    "Explain what these evaluation metrics mean for the specific task of cancer detection. Based on the observed metrics, comment on whether the fitted model is better than the all 0's classifier.\n",
    "> **In the context of cancer detection, the *true positive rate* represents the proportion of true cancer patients being classied as having cancer, and the *true negative rate* represents the proportion of true healthy subjects being classified as normal. For the specific task of cancer detection, the ability to diagnose cancer patients as having cancer is of paramount importance. Yet, the all 0's classifier has 0 chance of detecting cancer (TPR = 0) while the fitted model has a larger TPR of about 0.2. Therefore, even though the 2 clasffiers have similarly high true negative rates (`Specificity` $\\sim$ 1), the fitted model is better than the all 0's classifier.\n",
    "\n",
    "What is the *false positive rate* of the fitted classifier, and how is it related to its true positive and true negative rate? \n",
    "   \n",
    "> **False positive rate of the fitted classifier: = 0.0003531489111241907**\n",
    "\n",
    "> **False positive rate **\n",
    "\n",
    "> ** = $\\frac{false\\_positive}{observd\\_negative}$ **\n",
    "\n",
    ">  ** = $\\frac{observd\\_negative - true\\_negative}{observd\\_negative}$ **\n",
    "\n",
    ">  ** = $1- \\frac{true\\_negative}{observd\\_negative}$**\n",
    "\n",
    ">  ** = 1 - True Negative Rate **\n",
    "\n",
    "Why is a classifier with high false positive rate undesirable for a cancer detection task?\n",
    "> ** A cancer detection classifier with high false positive rate means that healthy subjects are classified as cancer patients at a high rate. This is undesirable because 1) hospitals would waste resources, 2) insurance companies would lose money and 3) perhaps more importantly, more subjects would go through unnecessary emotional trauma associated with a cancer diagosis, would have to spend a substantial amount of money on subsequent tests and would have increased health risks from getting more unnecessary tests. **\n",
    "\n",
    "> **(On the other hand, a cancer detection classifier with a high false negative rate is also undesirable because patients are not being diagnosed and put on appropriate treatment. By postponing diagnosis, morbidity and mortality rates would increase. )**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: ROC Analysis\n",
    "\n",
    "Another powerful diagnostic tool for class-imbalanced classification tasks is the Receiver Operating Characteristic (ROC) curve. Notice that the default logistic regression classifier in `sklearn` classifies a data point by thresholding the predicted class probability $\\hat{P}(Y=1)$ at 0.5. By using a different threshold, we can adjust the trade-off between the true positive rate (TPR) and false positive rate (FPR) of the classifier. The ROC curve allows us to visualize this trade-off across all possible thresholds.\n",
    "\n",
    "\n",
    "1. Display the ROC curve for the fitted classifier on the *test set*. In the same plot, also display the ROC curve for the all 0's classifier. How do the two curves compare?\n",
    "\n",
    "2.  Compute the highest TPR that can be achieved by the classifier at each of the following FPR's, and the thresholds at which they are achieved. Based on your results, comment on how the threshold influences a classifier's FPR.\n",
    "    - FPR = 0\n",
    "    - FPR = 0.1\n",
    "    - FPR = 0.5\n",
    "    - FPR = 0.9\n",
    "- Suppose a clinician told you that diagnosing a cancer patient as normal is *twice* as critical an error as diagnosing a normal patient as having cancer. Based on this information, what threshold would you recommend the clinician to use? What is the TPR and FPR of the classifier at this threshold? \n",
    "\n",
    "- Compute the area under the ROC curve (AUC) for both the fitted classifier and the all 0's classifier. How does the difference in the AUCs of the two classifiers compare with the difference between their classification accuracies in Question 1, Part 2(A)? \n",
    "\n",
    "*Hint:* You may use the `metrics.roc_curve` function to compute the ROC curve for a classification model and the `metrics.roc_auc_score` function to compute the AUC for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make ROC code from Lab7\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0, all_zero=False):\n",
    "    initial=False\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "        initial=True\n",
    "    if all_zero == False: \n",
    "        if proba:# for stuff like logistic regression\n",
    "            fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
    "            roc_auc = roc_auc_score(ytest, clf.predict_proba(xtest)[:,1])    \n",
    "        else:# for stuff like SVM\n",
    "            fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n",
    "    \n",
    "    else: # all_zero predictor\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, np.zeros(len(xtest)))\n",
    "        roc_auc = roc_auc_score(ytest, np.zeros(len(xtest)))\n",
    "        \n",
    "#     roc_auc = auc(fpr, tpr) # compute Area Under the Curve, same with roc_auc_score(ytest, yscore)\n",
    "\n",
    "    if skip:\n",
    "        l=fpr.shape[0] \n",
    "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', alpha=0.8, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    else:\n",
    "        ax.plot(fpr, tpr, '.-', alpha=0.8, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    \n",
    "    if all_zero:\n",
    "        ax.scatter(x=fpr, y=tpr, marker='o', s=400, color='r', label=' scatter - all_0\\'s_classifier')\n",
    "        \n",
    "    \n",
    "    label_kwargs = {}\n",
    "    label_kwargs['bbox'] = dict(\n",
    "        boxstyle='round,pad=0.3', alpha=0.2,\n",
    "    )\n",
    "    if labe!=None:\n",
    "        for k in range(0, fpr.shape[0],labe):\n",
    "            #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n",
    "            threshold = str(np.round(thresholds[k], 2))\n",
    "            ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n",
    "    if initial:\n",
    "        ax.plot([0, 1], [0, 1], 'k--')\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10d84e7f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyMAAAI1CAYAAADFKyX3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npZRSyi20MqKUUkoppZRyi/8HymPKuVLX25AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d0c7a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_context(\"poster\")\n",
    "ax = make_roc(\"logistic\", log_regr_cv, y_test_full, X_test_full, labe=None)\n",
    "make_roc(\"all_zero\", None, y_test_full, X_test_full, labe=None, all_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manually change binary clf threshold to t, from Lab7\n",
    "def t_repredict(est, t, xtest):\n",
    "    probs = est.predict_proba(xtest)\n",
    "    p0 = probs[:,0]\n",
    "    p1 = probs[:,1]\n",
    "    ypred = (p1 > t)*1\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR = 0\n",
      "Highest TPR = 0.08080808080808081\n",
      "p_threshold = 0.9405510934189472\n",
      "Confusion Matrix: \n",
      "[[16990     0]\n",
      " [   92     7]]\n",
      "------------------------\n",
      "\n",
      "FPR = 0.1\n",
      "Highest TPR = 0.8383838383838383\n",
      "p_threshold = 0.006266183038149889\n",
      "Confusion Matrix: \n",
      "[[15292  1698]\n",
      " [   16    83]]\n",
      "------------------------\n",
      "\n",
      "FPR = 0.5\n",
      "Highest TPR = 0.9696969696969697\n",
      "p_threshold = 0.00021027820533256583\n",
      "Confusion Matrix: \n",
      "[[8496 8494]\n",
      " [   3   96]]\n",
      "------------------------\n",
      "\n",
      "FPR = 0.9\n",
      "Highest TPR = 1.0\n",
      "p_threshold = 2.486773984223019e-06\n",
      "Confusion Matrix: \n",
      "[[ 1700 15290]\n",
      " [    0    99]]\n",
      "------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FPRs = [0, 0.1, 0.5, 0.9]\n",
    "fpr_long, tpr_long, thresholds_long = roc_curve(y_test_full, log_regr_cv.predict_proba(X_test_full)[:,1], drop_intermediate=False)\n",
    "\n",
    "for FPR in FPRs:\n",
    "    print('FPR = {}'.format(FPR))\n",
    "    for i, fpr in enumerate(fpr_long):\n",
    "        if fpr > FPR:\n",
    "            print('Highest TPR = {}'.format(tpr_long[i-1]))\n",
    "            print('p_threshold = {}'.format(thresholds_long[i-1]))\n",
    "            print('Confusion Matrix: ')\n",
    "            print(confusion_matrix(y_test_full, t_repredict(log_regr_cv, thresholds_long[i-1], X_test_full)))\n",
    "            print('------------------------\\n')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observed negatives = 16990 \n",
      "Number of observed positives = 99\n"
     ]
    }
   ],
   "source": [
    "n_observed_0 = sum(y_test_full==0)\n",
    "n_observed_1 = sum(y_test_full==1)\n",
    "print('Number of observed negatives = {} \\nNumber of observed positives = {}'.format(n_observed_0, n_observed_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given FN twice the cost of FP: \n",
      " - threshold = 0.384256642291652, \n",
      " - fpr = 0.0007062978222483814, \n",
      " - tpr = 0.2828282828282828\n"
     ]
    }
   ],
   "source": [
    "fpr_long, tpr_long, thresholds_long = roc_curve(y_test_full, log_regr_cv.predict_proba(X_test_full)[:,1], drop_intermediate=False)\n",
    "loss_long = []\n",
    "for fpr, tpr in zip(fpr_long, tpr_long):\n",
    "    loss = fpr + (1-tpr)*(n_observed_1/n_observed_0)*2 # define the loss taking a FN twice the cost of a FP\n",
    "    loss_long.append(loss)\n",
    "i_min_loss = np.argmin(loss_long)\n",
    "print('Given FN twice the cost of FP: \\n - threshold = {}, \\n - fpr = {}, \\n - tpr = {}'.format(thresholds_long[i_min_loss], fpr_long[i_min_loss], tpr_long[i_min_loss]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers for Question 2\n",
    "\n",
    "Display the ROC curve for the fitted classifier on the *test set*. In the same plot, also display the ROC curve for the all 0's classifier. How do the two curves compare?\n",
    "> **The fitted classifier has a smooth ROC curve, and as false positive rate increases, the true positive rate increases and approaches 1. On the other hand, the all 0's classfier is a straight diagonal line, consisting of only 2 end points at (0,0) & (1,1). **\n",
    "\n",
    "Compute the highest TPR that can be achieved by the classifier at each of the following FPR's, and the thresholds at which they are achieved. Based on your results, comment on how the threshold influences a classifier's FPR.\n",
    "> - **FPR = 0, $\\quad$   Highest TPR $\\sim$ 0.0808, $\\quad$ Threshold = 0.9405510934189472**\n",
    "> - **FPR = 0.1, $\\quad$ Highest TPR $\\sim$ 0.8384,  $\\quad$ Threshold = 0.006266183038149889**\n",
    "> - **FPR = 0.5, $\\quad$ Highest TPR $\\sim$ 0.9697, $\\quad$ Threshold = 0.00021027820533256583**\n",
    "> - **FPR = 0.9, $\\quad$ Highest TPR $\\sim$ 1.0, $\\quad$ Threshold = 2.486773984223019e-06**\n",
    "\n",
    "> **As summarized above, as FPR increases, the highest TPR increases and the threshold decreases. Given the logistic classifier trained on this dataset, the larger the threshold, the less false positives we can get. But large thresholds also limit TPR to be small. This makes the model less likely to predict positives (either false or true). Therefore, We will get more false negatives, which usally cost more than a false positive for the specific task of cancer detection. **\n",
    "\n",
    "\n",
    "Suppose a clinician told you that diagnosing a cancer patient as normal is *twice* as critical an error as diagnosing a normal patient as having cancer. Based on this information, what threshold would you recommend the clinician to use? What is the TPR and FPR of the classifier at this threshold? \n",
    "> **Given a false negative is twice as critical as a false positive, we define a loss to be the total cost including both false positives and false negatives. Each false negative prediction costs twice of each false positive prediction. That is**\n",
    "\n",
    "> **`loss = fpr + (1-tpr)*(n_observed_1/n_observed_0)*2` **\n",
    "\n",
    "> (assuming the ratio ** `n_observed_1/n_observed_0`** to be representative of the true population)\n",
    "\n",
    "> **Walking through all possible values of fpr, tpr and thresholds given by the ROC curve, we calculate this loss at each point. The recommended threshold is achived at the `np.argmin(loss_long)`-th point. **\n",
    "\n",
    "> - ** Recommended Threshold  = 0.384256642291652 **\n",
    "> - ** FPR at the Threshold = 0.0007062978222483814 **\n",
    "> - ** TPR at the Threshold = 0.2828282828282828 **\n",
    "\n",
    "Compute the area under the ROC curve (AUC) for both the fitted classifier and the all 0's classifier. How does the difference in the AUCs of the two classifiers compare with the difference between their classification accuracies in Question 1, Part 2(A)? \n",
    "> **Model Predicting All Zeros:**\n",
    "   - **area under ROC = 0.5**\n",
    "\n",
    "> **Fitted LogisticRegression Classifier:**\n",
    "   - **area under ROC = 0.93**\n",
    "\n",
    "> ** The difference in the AUCs of the two classifiers is much larger than the difference between their classification accuracies in Question 1 Part 2(a).**\n",
    "\n",
    "> ** A better model as a larger AUC. Therefore, judging by the AUC, the fitted classifier is much better than the all 0's classifier. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 3: Missing data\n",
    "\n",
    "In this problem you are given a different data set, `hw6_dataset_missing.csv`, that is  similar to the one you used above (same column definitions and same conditions), however this data set contains missing values. \n",
    "\n",
    "*Note*: be careful of reading/treating column names and row names in this data set as well, it *may* be different than the first data set.\n",
    "\n",
    "\n",
    "1. Remove all observations that contain and missing values, split the dataset into a 75-25 train-test split, and fit the regularized logistic regression as in Question 1 (use `LogisticRegressionCV` again to retune).  Report the overall classification rate and TPR in the test set.\n",
    "2. Restart with a fresh copy of the data in `hw6_dataset_missing.csv` and impute the missing data via mean imputation.  Split the data 75-25 and fit the regularized logistic regression model.  Report the overall classification rate and TPR in the test set.  \n",
    "3. Again restart with a fresh copy of the data in `hw6_dataset_missing.csv` and impute the missing data via a model-based imputation method. Once again split the data 75-25 and fit the regularized logistic regression model.  Report the overall classification rate and TPR in the test set.  \n",
    "4. Compare the results in the 3 previous parts of this problem.  Prepare a paragraph (5-6 sentences) discussing the results, the computational complexity of the methods, and conjecture and explain why you get the results that you see.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: missing values dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there any value missing in the last column (response): False\n",
      "X_train.shape=(1080, 117) \n",
      "X_test.shape=(356, 117) \n",
      "y_train.shape=(1080,) \n",
      "y_test=shape(356,)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw missing data\n",
    "df_mis = pd.read_csv('HW6_dataset_missing.csv', index_col='Unnamed: 0')\n",
    "print('Is there any value missing in the last column (response): {}'.format(df_mis[df_mis.columns[117]].isnull().any()))\n",
    "\n",
    "# Remove all missing values \n",
    "df_mis_drop = df_mis.dropna(how = 'any')\n",
    "\n",
    "# Split data into 0.75 Train & 0.25 Test\n",
    "X_train_drop, X_test_drop, y_train_drop, y_test_drop = split_hw6_data(df_mis_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "LogisticRegressionCV w/ missing values DROPPED\n",
      "-----------------------------------\n",
      "Accuracy - Test: 0.9943820224719101\n",
      "Confusion Matrix:\n",
      "[[354   0]\n",
      " [  2   0]]\n",
      "True Positive Rate: 0.0\n",
      "True Negative Rate: 1.0\n",
      "False Positive Rate: 0.0\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasminetong/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# Fit LogistricRegressionCV with L2 regularization on the FULL dataset\n",
    "log_regr_drop = LogisticRegressionCV(penalty='l2', scoring='accuracy')\n",
    "log_regr_drop.fit(X_train_drop, y_train_drop)\n",
    "log_drop_score_test = log_regr_drop.score(X_test_drop, y_test_drop)\n",
    "log_drop_cm = confusion_matrix(y_test_drop, log_regr_drop.predict(X_test_drop))\n",
    "\n",
    "model_summary('LogisticRegressionCV w/ missing values DROPPED', log_drop_score_test, log_drop_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: missing values imputed with column mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Imputation Duration = 0.03406786918640137 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the raw missing data\n",
    "df_imp_mean = pd.read_csv('hw6_dataset_missing.csv', index_col='Unnamed: 0')\n",
    "\n",
    "# Fill NaNs with column mean\n",
    "start_imp_mean = time.time()\n",
    "df_imp_mean.fillna(df_imp_mean.mean()[:117], inplace=True) \n",
    "print('Mean Imputation Duration = {} seconds'.format(time.time() - start_imp_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=(18874, 117) \n",
      "X_test.shape=(6125, 117) \n",
      "y_train.shape=(18874,) \n",
      "y_test=shape(6125,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into 0.75 Train & 0.25 Test\n",
    "X_train_imp_mean, X_test_imp_mean, y_train_imp_mean, y_test_imp_mean = split_hw6_data(df_imp_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "LogisticRegressionCV w/ missing values IMPUTED WITH MEAN\n",
      "-----------------------------------\n",
      "Accuracy - Test: 0.9926530612244898\n",
      "Confusion Matrix:\n",
      "[[6070    4]\n",
      " [  41   10]]\n",
      "True Positive Rate: 0.19607843137254902\n",
      "True Negative Rate: 0.9993414553836022\n",
      "False Positive Rate: 0.0006585446163977609\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# Fit LogistricRegressionCV with L2 regularization on the MEAN IMPUTED dataset\n",
    "log_regr_imp_mean = LogisticRegressionCV(penalty='l2', scoring='accuracy')\n",
    "log_regr_imp_mean.fit(X_train_imp_mean, y_train_imp_mean)\n",
    "log_imp_mean_score_test = log_regr_imp_mean.score(X_test_imp_mean, y_test_imp_mean)\n",
    "log_imp_mean_cm = confusion_matrix(y_test_imp_mean, log_regr_imp_mean.predict(X_test_imp_mean))\n",
    "\n",
    "model_summary('LogisticRegressionCV w/ missing values IMPUTED WITH MEAN', log_imp_mean_score_test, log_imp_mean_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3.1: missing values imputed with a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns that have any missing value: 17\n",
      "Columns with NaNs:\n",
      "['93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109']\n"
     ]
    }
   ],
   "source": [
    "n_col_mis = sum(df_mis.isnull().any())\n",
    "print('Number of columns that have any missing value: {}'.format(n_col_mis))\n",
    "\n",
    "mis_col_names = []\n",
    "full_col_names = []\n",
    "for col in df_mis.columns[:117]:\n",
    "    if df_mis[col].isnull().any():\n",
    "        mis_col_names.append(col) # Names of columns with NaNs \n",
    "    else:\n",
    "        full_col_names.append(col) # Names of columns without NaNs \n",
    "\n",
    "print('Columns with NaNs:')\n",
    "print(mis_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasminetong/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Imputation Duration = 3.389396905899048 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make copies of the dataset with missing values\n",
    "df_mis = pd.read_csv('HW6_dataset_missing.csv', index_col='Unnamed: 0')\n",
    "df_imp_lin = pd.read_csv('HW6_dataset_missing.csv', index_col='Unnamed: 0')\n",
    "lin_regr = LinearRegression()\n",
    "\n",
    "start_imp_lin = time.time()\n",
    "for col in mis_col_names:\n",
    "    df = df_mis[full_col_names] # copy the cols without NaNs\n",
    "    df[col] = df_mis[col] # append the col with NaNs\n",
    "    df = df.dropna(axis=0, subset=[col]) # drop the rows with NaNs in col\n",
    "    \n",
    "    X_full = df[full_col_names]\n",
    "    y_full = df[col]\n",
    "    lin_regr.fit(X_full, y_full) # fit linear regression for the col with NaNs using other full cols\n",
    "    y_hat = lin_regr.predict(X_full)\n",
    "    mse = mean_squared_error(y_full, y_hat) # the current model's MSE\n",
    "    \n",
    "    X_missing = df_mis[df_mis[col].isnull()][full_col_names]\n",
    "    y_missing = lin_regr.predict(X_missing)\n",
    "    \n",
    "    # Add random uncertainty (scale = the current model's MSE )\n",
    "    y_missing_noise = y_missing + np.random.normal(loc=0, scale=np.sqrt(mse), size=y_missing.shape[0])\n",
    "    \n",
    "    missing_index = df_mis[col][df_mis[col].isnull()].index # get indices of the missing values\n",
    "    missing_series = pd.Series(data = y_missing_noise, index = missing_index)\n",
    "\n",
    "    df_imp_lin[col].fillna(missing_series, inplace=True) # fill in NaNs with linear regression results\n",
    "\n",
    "print('Logistic Regression Imputation Duration = {} seconds'.format(time.time() - start_imp_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=(18874, 117) \n",
      "X_test.shape=(6125, 117) \n",
      "y_train.shape=(18874,) \n",
      "y_test=shape(6125,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into 0.75 Train & 0.25 Test\n",
    "X_train_imp_lin, X_test_imp_lin, y_train_imp_lin, y_test_imp_lin = split_hw6_data(df_imp_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "LogisticRegressionCV w/ missing values IMPUTED WITH LINEAR REGRESSION\n",
      "-----------------------------------\n",
      "Accuracy - Test: 0.9929795918367347\n",
      "Confusion Matrix:\n",
      "[[6073    1]\n",
      " [  42    9]]\n",
      "True Positive Rate: 0.17647058823529413\n",
      "True Negative Rate: 0.9998353638459005\n",
      "False Positive Rate: 0.00016463615409944023\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# Fit LogistricRegressionCV with L2 regularization on the LINEAR REGRESSIOPN IMPUTED dataset\n",
    "log_regr_imp_lin = LogisticRegressionCV(penalty='l2', scoring='accuracy')\n",
    "log_regr_imp_lin.fit(X_train_imp_lin, y_train_imp_lin)\n",
    "log_imp_lin_score_test = log_regr_imp_lin.score(X_test_imp_lin, y_test_imp_lin)\n",
    "log_imp_lin_cm = confusion_matrix(y_test_imp_lin, log_regr_imp_lin.predict(X_test_imp_lin))\n",
    "\n",
    "model_summary('LogisticRegressionCV w/ missing values IMPUTED WITH LINEAR REGRESSION', log_imp_lin_score_test, log_imp_lin_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3.2: missing values imputed with a KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasminetong/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN(k=3) Regression Imputation Duration = 345.2554728984833 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make copies of the dataset with missing values\n",
    "df_mis = pd.read_csv('HW6_dataset_missing.csv', index_col='Unnamed: 0')\n",
    "df_imp_knn = pd.read_csv('HW6_dataset_missing.csv', index_col='Unnamed: 0')\n",
    "knn_regr = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "start_imp_knn = time.time()\n",
    "for col in mis_col_names:\n",
    "    df = df_mis[full_col_names] # copy the cols without NaNs\n",
    "    df[col] = df_mis[col] # append the col with NaNs\n",
    "    df = df.dropna(axis=0, subset=[col]) # drop the rows with NaNs in col\n",
    "    \n",
    "    X_full = df[full_col_names]\n",
    "    y_full = df[col]\n",
    "    \n",
    "    # build models with cross validation to find the optimal k (taking too long time on my Mac)\n",
    "#     cv_scores = []\n",
    "#     for k in ks:\n",
    "#         knn_regr.set_params(n_neighbors=k)\n",
    "#         scores = cross_val_score(knn_regr, X_full, y_full, cv=3, scoring='r2')\n",
    "#         cv_scores.append(scores.mean())\n",
    "        \n",
    "#     optimal_k = ks[np.argmax(cv_scores)]\n",
    "#     knn_regr.set_params(n_neighbors=optimal_k) # set the number of neighbors to the optimal k\n",
    "    \n",
    "    knn_regr.fit(X_full, y_full)\n",
    "    \n",
    "    X_missing = df_mis[df_mis[col].isnull()][full_col_names]\n",
    "    y_missing = knn_regr.predict(X_missing)\n",
    "    \n",
    "    missing_index = df_mis[col][df_mis[col].isnull()].index # get indices of the missing values\n",
    "    missing_series = pd.Series(data = y_missing, index = missing_index)\n",
    "\n",
    "    df_imp_knn[col].fillna(missing_series, inplace=True) # fill in NaNs with linear regression results\n",
    "\n",
    "print('KNN(k=3) Regression Imputation Duration = {} seconds'.format(time.time() - start_imp_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=(18874, 117) \n",
      "X_test.shape=(6125, 117) \n",
      "y_train.shape=(18874,) \n",
      "y_test=shape(6125,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into 0.75 Train & 0.25 Test\n",
    "X_train_imp_knn, X_test_imp_knn, y_train_imp_knn, y_test_imp_knn = split_hw6_data(df_imp_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "LogisticRegressionCV w/ missing values IMPUTED WITH KNN(k=3) REGRESSION\n",
      "-----------------------------------\n",
      "Accuracy - Test: 0.9929795918367347\n",
      "Confusion Matrix:\n",
      "[[6073    1]\n",
      " [  42    9]]\n",
      "True Positive Rate: 0.17647058823529413\n",
      "True Negative Rate: 0.9998353638459005\n",
      "False Positive Rate: 0.00016463615409944023\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# Fit LogistricRegressionCV with L2 regularization on the KNN IMPUTED dataset\n",
    "log_regr_imp_knn = LogisticRegressionCV(penalty='l2', scoring='accuracy')\n",
    "log_regr_imp_knn.fit(X_train_imp_knn, y_train_imp_knn)\n",
    "log_imp_knn_score_test = log_regr_imp_knn.score(X_test_imp_knn, y_test_imp_knn)\n",
    "log_imp_knn_cm = confusion_matrix(y_test_imp_knn, log_regr_imp_knn.predict(X_test_imp_knn))\n",
    "\n",
    "model_summary('LogisticRegressionCV w/ missing values IMPUTED WITH KNN(k=3) REGRESSION', log_imp_knn_score_test, log_imp_knn_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers for Question 3\n",
    "\n",
    "Report the overall classification rate **(test_acc)** and TPR **(test_TPR)** in the test set.\n",
    "\n",
    "**Missing values DROPPED**\n",
    "> - ** test_acc = 0.9943820224719101 **\n",
    "> - ** test_TPR = 0.0 **\n",
    "\n",
    "**Missing values IMPUTED WITH MEAN**\n",
    "> - ** test_acc = 0.9926530612244898 **\n",
    "> - ** test_TPR = 0.19607843137254902 **\n",
    " \n",
    "**Missing values IMPUTED WITH LINEAR REGRESSION**\n",
    "> - ** test_acc = 0.9929795918367347 **\n",
    "> - ** test_TPR = 0.17647058823529413 **\n",
    "\n",
    "**Missing values IMPUTED WITH KNN(k=3) REGRESSION**\n",
    "> **Same with Missing values IMPUTED WITH LINEAR REGRESSION**\n",
    "> - ** test_acc = 0.9929795918367347 **\n",
    "> - ** test_TPR = 0.17647058823529413 **\n",
    "\n",
    "\n",
    "Compare the results in the 3 previous parts of this problem.  Prepare a paragraph (5-6 sentences) discussing the results, the computational complexity of the methods, and conjecture and explain why you get the results that you see.\n",
    "\n",
    "> **In Terms of Classification Accuracy: **\n",
    "> > **All of the models have similarly high accuracy on the test set (the biggest difference is only ~0.0017. One of the reasons for such high accuracy across all models is because the classes are very imbalanced. We have a lot more data points with a true '0' label compared to data points labeled '1'. Trained on highly imbalanced data, all models tend to predict '0''s correctly, leading to high classification accuracy. **\n",
    "\n",
    "> > ** Simply dropping missing values gives comparatively the best classification accuracy. This is because dropping missing values further increases the percentage of true '1' observations, while imputation would not change this percentage. For a class-imbalanced classification task, the fitted logistic model would naturally have higher accuracy when the percentage of true '1' observations is higher.**\n",
    "\n",
    "> **In Terms of Ture Postive Rate: **\n",
    "> > **A more reliable metric for model performance on class-imbalanced classification tasks is TPR on test set. The ranking of our models in terms of test_TPR is: MEAN imputation > LINEAR regression imputation = KNN(k=3) regression imputation > DROPPING all the missing observations. One potential reason why mean imputation performed better here compared to model imputation is that the predictors (columns) with missing values are not explained by the other predictors well. Furthermore, the predictors with missing values are likely to have a symmetric distribution so that mean imputation models well. Therefore, using other predictors to predict missing values for this dataset is not as effective as simply imputing with the mean.**\n",
    "\n",
    "> **In Terms of Model Complexity: **\n",
    "> - ** Running time (seconds) of the three imputation methods: **\n",
    "    > - ** Mean Imputation: ~ 0.035 **\n",
    "    > - ** Linear Regression Imputation : ~ 3.5 **\n",
    "    > - ** KNN(k=3) Imputation: ~ 350 **\n",
    "    \n",
    "> > ** Therefore, the ranking of our models in terms of model complexity is: KNN(k=3) regression imputation >> LINEAR regression imputation > MEAN imputation > DROPPING all the missing observations.**\n",
    "\n",
    "> > ** In conclusion, since mean imputation has the best TPR and low model complexity, we should adopt mean imputation for this dataset.  One caveat with mean imputation is that the sample variance within the predictors imputed would be smaller than expected, which should be taken into consideration (and potentially adjusted with a factor) when we use statistical tests to test for significance of predictors. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APCOMP209a - Homework Question\n",
    "## Answers inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "This problem walks you through the derivation of the **likelihood equations** for a generalized linear model (GLM). Suppose that the random component of the GLM is in the univariate natural exponential family, so that\n",
    "$$f(y_i|\\theta_i) = h(y_i) e^{y_i\\theta_i - b(\\theta_i)}$$\n",
    "Define the individual log-likelihood for each observation $i$ as\n",
    "$$l_i(\\theta_i) \\equiv \\log f(y_i|\\theta_i)$$\n",
    "with linear predictor\n",
    "$$\\eta_i = x_i^T\\beta = g(\\mu_i)$$\n",
    "for some link function $g$ and where $\\mu_i=E(Y_i)$.\n",
    "\n",
    "1. Use the above expressions to write a simplified expression for the log-likelihood $l(\\theta)$ for the entire dataset, $y_1, \\dots, y_n$.\n",
    "\n",
    "    > **Answer 1** $$ l(\\theta) = \\log [\\Pi _i f(y_i|\\theta _i)] = \\Sigma _i [\\log f(y_i|\\theta _i)] = \\Sigma _i l_i(\\theta _i) $$\n",
    "\n",
    "2. Use the chain rule to express $\\frac{\\partial l_i}{\\partial \\beta_j}$ in terms of the derivatives of $l_i, \\theta_i, \\mu_i$, and $\\eta_i$. (*Hint*: Think carefully about which variables are related to which, and in what way. For example, for which of the above variables do you know the derivative with respect to $\\beta_j$?)\n",
    "\n",
    "    > **Answer 2** \n",
    "    \n",
    "    > Dependence Chain =\n",
    "    \n",
    "    > (loss_function vs. natrual_parameter $\\theta $) \n",
    "    \n",
    "    > x (natrual_parameter $\\theta $ vs. mean) \n",
    "    \n",
    "    > x (mean vs. link_function ) \n",
    "    \n",
    "    > x (linear_predictor_components) \n",
    "    \n",
    "    > $$ \\frac{\\partial l_i}{\\partial \\beta_j} = \\frac{\\partial l_i}{\\partial \\theta _i} \\frac{\\partial \\theta _i}{\\partial \\mu _i} \\frac{\\partial \\mu _i}{\\partial \\eta _i} \\frac{\\partial \\eta _i}{\\partial \\beta_j} $$\n",
    "\n",
    "3. Compute the derivatives for $\\frac{\\partial l_i}{\\partial \\theta_i}$ and $\\frac{\\partial \\eta_i}{\\partial \\beta_j}$.\n",
    "\n",
    "    > **Answer 3** $$ \\frac{\\partial l_i}{\\partial \\theta_i} = \\frac{1}{f(y_i|\\theta _i)} \\frac{\\partial f(y_i|\\theta _i)}{\\partial \\theta _i} =  \\frac{1}{f(y_i|\\theta _i)} f(y_i|\\theta _i) (y_i-b^{'}(\\theta _i)) = y_i-b^{'}(\\theta _i) = y_i - \\mu _i $$\n",
    "    \n",
    "    > $$\\frac{\\partial \\eta_i}{\\partial \\beta_j} = x_{ij} $$\n",
    "\n",
    "4. Express $\\mu_i$ in terms of $\\theta_i$, and use this relationship to compute $\\frac{\\partial \\theta_i}{\\partial \\mu_i}$. ({Hint}: Recall the cumulant function of a natural exponential family, and assume that you can write $\\partial f/\\partial g = (\\partial g / \\partial f)^{-1}$.)\n",
    "\n",
    "    > **Answer 4** $$ \\mu _i = b^{'}(\\theta _i) $$\n",
    "    \n",
    "    > $$ \\frac{\\partial \\theta_i}{\\partial \\mu_i} = \\frac{1}{\\frac{\\partial \\mu_i}{\\partial \\theta _i}} = \\frac{1}{b^{''}(\\theta _i)}$$\n",
    "\n",
    "5. Express $\\eta_i$ in terms of $\\mu_i$. Using the same hint as the above, compute $\\frac{\\partial \\mu_i}{\\partial \\eta_i}$.\n",
    "\n",
    "    > **Answer 5** $$ \\eta _i = g(\\mu _i) $$\n",
    "    \n",
    "    > $$ \\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{1}{\\frac{\\partial \\eta_i}{\\partial \\mu_i}} = \\frac{1}{g^{'}(\\mu_i)} $$\n",
    "\n",
    "6. Put all of the above parts together to write an expression for $\\frac{\\partial l}{\\partial \\beta_j}$. Use matrix notation to write this expression as $$\\nabla_{\\beta} l(\\beta) = XDV^{-1}(Y - \\mu) = 0$$ That is, compute the matrices $D$ and $V$ such that this equation holds.\n",
    "    \n",
    "    > **Answer 6** $$ \\frac{\\partial l}{\\partial \\beta_j} =  \\frac{\\partial l_i}{\\partial \\theta _i} \\frac{\\partial \\theta _i}{\\partial \\mu _i} \\frac{\\partial \\mu _i}{\\partial \\eta _i} \\frac{\\partial \\eta _i}{\\partial \\beta_j} = (y_i - \\mu _i) \\frac{1}{b^{''}(\\theta _i)} \\frac{1}{g^{'}(\\mu_i)} x_{ij} $$\n",
    "    \n",
    "    > $$ \\nabla_{\\beta} l(\\beta) = XDV^{-1}(Y - \\mu) = 0 $$\n",
    "    \n",
    "    > $$ D = \\begin{bmatrix}\n",
    "        \\frac{1}{g^{'}(\\mu_1)} & 0 & \\ldots & 0 \\\\\n",
    "        0 & \\frac{1}{g^{'}(\\mu_2)} & \\ldots & 0 \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\vdots & \\\\\n",
    "        0 & \\ldots & 0 & \\frac{1}{g^{'}(\\mu_n)} \\\\\n",
    "        \\end{bmatrix} $$\n",
    "    \n",
    "    > $$ V = \\begin{bmatrix}\n",
    "        b^{''}(\\theta_1) & 0 & \\ldots & 0 \\\\\n",
    "        0 & b^{''}(\\theta_2) & \\ldots & 0 \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\vdots & \\\\\n",
    "        0 & \\ldots & 0 & b^{''}(\\theta_n) \\\\\n",
    "        \\end{bmatrix} \\quad\n",
    "        V^{-1} = \\begin{bmatrix}\n",
    "        \\frac{1}{b^{''}(\\mu_1)} & 0 & \\ldots & 0 \\\\\n",
    "        0 & \\frac{1}{b^{''}(\\mu_2)} & \\ldots & 0 \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\vdots & \\\\\n",
    "        0 & \\ldots & 0 & \\frac{1}{b^{''}(\\mu_n)} \\\\\n",
    "        \\end{bmatrix} $$\n",
    "\n",
    "7. If we use the canonical link function, how do your answers to part (6) simplify?\n",
    "\n",
    "    > **Answer 7** Canonical Link function gives $$ \\theta _i = x_i^{T} \\beta = \\eta_i $$.\n",
    "    > $$ \\frac{\\partial \\theta_i}{\\partial \\mu_i} \\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{\\partial \\theta_i}{\\eta_i} = 1$$\n",
    "    \n",
    "    > $$ D V^{-1} = I $$\n",
    "    \n",
    "    > $$ \\nabla_{\\beta} l(\\beta) = XDV^{-1}(Y - \\mu) = X(Y - \\mu) = 0 $$\n",
    "\n",
    "8. Finally, compute the above likelihood equations in the case of logistic regression, and show that this is equivalent to the solution given in lecture.\n",
    "\n",
    "    > **Answer 8** The above likelihood equations give $$Y = \\mu$$\n",
    "    \n",
    "    > $$f(y_i|\\theta) = h(y_i) e^{y_i\\theta_i - b(\\theta_i)}$$\n",
    "    \n",
    "    > In Logistic Regression, $$f(y_i|\\theta_i) = \\mu_i^{y_i} (1-\\mu_i)^{1-y_i} = exp[y_i\\ln \\frac{\\mu _i}{1-\\mu _i}+\\ln (1-\\mu_i)]$$.\n",
    "    \n",
    "    > Therefore, $$\\theta_i = \\ln \\frac{\\mu_i}{1-\\mu_i}$$\n",
    "    \n",
    "    > Plug in the solution $Y = \\mu$ and change the equation to represent matrix, we get \n",
    "    \n",
    "    > $$ \\theta = \\ln \\frac{Y}{1-Y}$$\n",
    "    \n",
    "    > $$ \\frac{Y}{1-Y} = e^{\\theta} = e^{X^T\\beta}$$\n",
    "    \n",
    "    > $$ Y = \\frac{e^{X^{T}\\beta}}{1+e^{X^{T}\\beta}}$$ Y represents P(Y=1) in the lecture.\n",
    "    \n",
    "    > This proves that the solution of the likelihood equations is the same with the formula for P(Y=1) given in the lecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
